---
title: "Bids Conversion"
author: "Anastasios Dadiotis"
editor: visual
toc: true
format:
  html:
    code-tools: true
    self-contained: true
---

# Introduction

This document is a step-by-step guide on how to convert raw data from a study to [BIDS](https://bids.neuroimaging.io/) format.[^1] The tool that is used here is [HeuDiConv](https://neuroimaging-core-docs.readthedocs.io/en/latest/pages/heudiconv.html). The data used in this example is from the Tiger study and already downloaded from xnat (see - add link when right this thing). The data are in a dicom format. Running HeuDiConv is a 3 step procedure. Note that this procedure takes place to the crnl cluster so this code is to be used with the cluster terminal and slurm. Most of the code is written in bash, but there are some python scripts that needs to be modified manually for the Bids conversion.

[^1]: For an excellent tutorial for Bids conversion [see](https://sarenseeley.github.io/BIDS-fmriprep-MRIQC.html).

# Bids Conversion

## Setup

In this preliminary step we setup and define our variables. Note from the code below, change the variable "subject" to the subject you are working on. The Tiger notation is C for controls and G for Gamblers, e.g. for the first control subject the notation is C01 and for the first gambler G01. The following procedure is to submit the job for one subject. (to be updated for more).

``` bash
# Standard variable for the Tiger study
WD=/mnt/data/anastasios_d
my_study=Tiger
session_name=S01

# change this to the subject you are working on
subject=XXX 
```

## [Step 1](https://neuroimaging-core-docs.readthedocs.io/en/latest/pages/heudiconv.html#heudiconv-step1): Generate a heuristic.py file.

By passing some path information and flags to HeuDiConv, you generate a heuristic (translation) file skeleton and some associated descriptor text files. These all get placed in a **hidden** directory, .heudiconv under the bids/derivatives directory.

``` bash
cd ${WD}/${my_study}/code
sbatch step1_heudiconv.sh ${subject} ${session_name} ${WD} ${my_study} 
squeue
```

To check that it worked the **out_step1\_{jobnumber}.log** file should have finished with **exit code 0**. Also, go to the /mnt/data/anastasios/Tiger/data/bids/derivatives/.heudiconv/{subject}/info were 5 files should be created.

Note that this is a **hidden** directory. To see it you need to type `ls -a` in the terminal. The file that we need for the next step is the **dicominfo.tsv file**.

## [Step 2](https://neuroimaging-core-docs.readthedocs.io/en/latest/pages/heudiconv.html#heudiconv-step2): Modify the heuristic.py file

You will modify the heuristic.py to specify BIDS output names and directories, and the input DICOM characteristics. Available input DICOM characteristics are listed in /.heudiconv/info/dicominfo.tsv.

Check the template of the heuristic.py file specifically for the Tiger study in the following path: /crnldata/psyr2/Anastasios/Tiger_fmri/Tiger/code/heuristic_template_Tiger.py. This is the one you should modify based on the dicominfo.tsv file of the specific subject.

The template is almost ready. What you should modify/check are the following lines:

1.  Line 122 s.dim4 == XXXX \# of volumes for the hariri task
2.  Line 128 s.dim == XXXX \# of volumes for the BigBuckBunny task
3.  Line 134 s.dim == XXXX \# of volumes for the PartlyCloudy task

The rest should remain the shame. But do check the following: 4. For the gambling task check that for each run \# of volumes \>0 5. For Fieldmaps check that the that the series_id has the correct number in it. (e.g. "9-gre_field_mapping" should map to the gambling task - adhust accordingly) 6. Diffusion should be ok as is. But do check AP - PA are in order/

At the end of this page there is the the heuristic.py file for inspection if nececarry.

## [Step 3](https://neuroimaging-core-docs.readthedocs.io/en/latest/pages/heudiconv.html#heudiconv-step3): Run HeuDiConv

Now that the heuristic.py file is ready we can run the HeuDiConv. Each time you run it, additional subdirectories are created under .heudiconv that record the details of each subject (and session) conversion. Detailed provenance information is retained in the .heudiconv hidden directory. The following code is to be run in the terminal.

``` bash
cd ${WD}/${my_study}/code
chmod -R 770 .
sbatch step3_heudiconv.sh ${subject} ${session_name} ${WD} ${my_study}
squeue
```

# Bids Validation

After the conversion is done, it is important to validate the Bids format. This is done by using the [BIDS validator](https://bids-standard.github.io/bids-validator/).

# Scripts

## step1_heudiconv.sh

``` {.bash filename="step1_heudiconv.sh" code-line-numbers="true" code-fold="true"}
#!/bin/bash

###############################
### Original script from Arnaud Fournel, PhD, NeuroPop team, CRNL, Lyon
### arnaud.fournel @ inserm.fr
### 
### Adapted for the CRNL study
### by Gaelle Leroux, PhD
### and Isabelle Faillenot, PhD
###
### Autumn 2020, Lyon
### gaelle.leroux @ cnrs.fr
###
### launched by: sbatch step1_heudiconv.sh ${subject} ${session_name} ${WD} ${my_study}
###
###############################
#
### The SBATCH directives (line 39 to be revised only):

### Your job name displayed by the queue
### use "squeue" command in a terminal to see it
#SBATCH --job-name=HeuDC_1

### Specify output and error files
### %A for job array's master job allocation number
### or %a for job array ID (index) number
#SBATCH --output=out_step1_%A.log
#SBATCH --error=err_step1_%A.log

### Specify the number of tasks, CPU per task and buffer size to be used 
### (up to 4 CPU/task to reach optimal power)
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=10G

### The size of the participant table to be run
###SBATCH --array=1-1

### If you want to launch your script on a specific node of the cluser
### If yes, uncomment the appropirate line(s) 
###SBATCH --exclude=node9
###SBATCH --nodelist=node10

# End of the SBATCH directives
###############################
#
# Paths of the study
subject=$1
session_name=$2
WD=$3
my_study=$4
STUDY=${WD}/${my_study}
DATA_DIRECTORY="${STUDY}/data"
#
HEUDICONV_SINGULARITY_IMG="/mnt/data/soft/Images/heudiconv_1.1.0.sif"
#
## Comment: Parse the participants.tsv file and extract one subject ID from the line corresponding to this SLURM task.
## Comment: 1 line below to uncomment if you want to launch the script for all the subjects listed in ${DATA_DIRECTORY}/participants.tsv
#subject=$( sed -n -E "$((${SLURM_ARRAY_TASK_ID} + 1))s/sub-(\S*)\>.*/\1/gp" ${DATA_DIRECTORY}/participants.tsv )
#
# To be printed in the out_*.log file :
echo "#########################################################################"
echo "User:" $USER
echo "#"
echo "SLURM_SUBMITING_DIRECTORY:" $SLURM_SUBMIT_DIR
echo "SLURM_JOB_NODELIST:" $SLURM_NODELIST
echo "SLURM_JOB_NAME:" $SLURM_JOB_NAME
echo "SLURM_JOB_ID:" $SLURM_JOBID
echo "SLURM_ARRAY_TASK_ID:" $SLURM_ARRAY_TASK_ID
echo "SLURM_NTASKS:" $SLURM_NTASKS
echo "#"
echo "Step 1: generation of text files using HeuDiConv"
echo "#"
echo "Subject processed:" ${subject}
echo "Session processed:" ${session_name}
echo "#"
echo "Job STARTED @ $(date)"
echo "#"
#
# STEP 1/3: generate heuristic file based on a template file + 4 other text files
# Submission to slurm and running the HeuDiConv singularity image
#
# Check the "-d" path (line 83) pointing at the dicom files
#
# Compose the command line
cmd="srun singularity run \
    --cleanenv \
    -B ${DATA_DIRECTORY}:/base \
    ${HEUDICONV_SINGULARITY_IMG} \
    -d /base/dicom/{subject}/{session}/scans/*/resources/DICOM/files/*.??? \
    -s ${subject} \
    --ses ${session_name} \
    -f convertall \
    -c none \
    -o /base/bids/derivatives \
    --overwrite"
#
# Setup done, run the command
echo Running task ${SLURM_ARRAY_TASK_ID}
echo "Command line used (example of the last subject processed)"
echo $cmd
#
echo "#"
#
eval $cmd
exitcode=$?
#
echo "#"
echo "Job STOPPED @ $(date)"
echo "#"
echo "If STEP 1 successful: 5 text files were generated in" ${STUDY}"/data/bids/derivatives/.heudiconv/"$1"/info"
echo "#"
echo "Now, STEP 2: edit the empty file heuristitic.py just created and copy/paste it to" ${STUDY}"/code"
echo "#"
echo "An example of heuristic file for a study@primage is provided in" ${STUDY}"/code/heuristic_templates/heuristic.py"
echo "#########################################################################"
echo "#"
# For your information about step 2:
# The heuristic file controls how information about the dicoms is used to convert to a file system layout (e.g., BIDS). 
# This is a python file that must have the function infotodict, which takes a single argument seqinfo.
#
# Output results to a table
echo "sub-$subject  ${SLURM_ARRAY_TASK_ID} $exitcode" >> ${SLURM_JOB_NAME}.step1.${SLURM_ARRAY_JOB_ID}.tsv
echo Finished tasks ${SLURM_ARRAY_TASK_ID} with exit code $exitcode
exit $exitcode
```

## heuristic.py

``` {.python filename="heuristic.py" code-line-numbers="true" code-fold="true"}
from __future__ import annotations

import logging
from typing import Optional

from heudiconv.utils import SeqInfo

lgr = logging.getLogger("heudiconv")


def create_key(
    template: Optional[str],
    outtype: tuple[str, ...] = ("nii.gz",),
    annotation_classes: None = None,
) -> tuple[str, tuple[str, ...], None]:
    if template is None or not template:
        raise ValueError("Template must be a valid format string")
    return (template, outtype, annotation_classes)


def infotodict(
    seqinfo: list[SeqInfo],
) -> dict[tuple[str, tuple[str, ...], None], list[str]]:
    """Heuristic evaluator for determining which runs belong where

    allowed template fields - follow python string module:

    item: index within category
    subject: participant id
    seqitem: run number during scanning
    subindex: sub index within group
    """

    # "data" creates sequential numbers which can be for naming sequences.
    # This is especially valuable if you run the same sequence multiple times at the scanner.
    data = create_key('run-{item:03d}')

    # Anatomical images
    # Structural scans (anat specification): MUST end with "T1w" or "T2w" or "FLAIR" or "T1map"...
    # list: https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/01-magnetic-resonance-imaging-data.html#anatomy-imaging-data
    t1w = create_key('sub-{subject}/{session}/anat/sub-{subject}_{session}_T1w')

    # Functional images
    # Tasks, including movies (func specification): MUST contain "task-" in the name + "bold" or "sbref" or "cbv" or "phase" at the end
    # list: https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/01-magnetic-resonance-imaging-data.html#task-including-resting-state-imaging-data
    # For tasks and movies, we need to specify the task name in the file name and then all the tasks are in the func folder. 

    # Big Buck Bunny task
    boldBigBuckBunny = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-BigBuckBunny_bold')
    boldBigBuckBunny_sbref = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-BigBuckBunny_sbref')

    # Hariri task
    boldHariri = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-Hariri_bold')
    boldHariri_sbref = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-Hariri_sbref')

    # Partly Cloudy task
    boldPartlyCloudy = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-PartlyCloudy_bold')
    boldPartlyCloudy_sbref = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-PartlyCloudy_sbref')   

    # Skewed gambling task NOTE: this task has multiple runs
    boldSkewedGambling = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-SkewedGambling_run-{item:02d}_bold')
    boldSkewedGambling_sbref = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-SkewedGambling_run-{item:02d}_sbref')


    # field maps (fmap specification): the file name must end with "magnitude" or "phasediff" and include the {subject}
    # specifications: https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/01-magnetic-resonance-imaging-data.html#fieldmap-data
    fmap_magn = create_key('sub-{subject}/{session}/fmap/sub-{subject}_{session}_magnitude')
    fmap_phase = create_key('sub-{subject}/{session}/fmap/sub-{subject}_{session}_phasediff')

    # Diffusion scans (dwi speccification): MUST end with "dwi"
    # specifications: https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/01-magnetic-resonance-imaging-data.html#diffusion-imaging-data
    dwi = create_key('sub-{subject}/{session}/dwi/sub-{subject}_{session}_acq-{acq}_dir-{dir}_dwi')
    dwi_sbref = create_key('sub-{subject}/{session}/dwi/sub-{subject}_{session}_acq-{acq}_dir-{dir}_sbref')


    info: dict[tuple[str, tuple[str, ...], None], list[str]] = {data: [],
                                                                boldHariri: [],
                                                                boldHariri_sbref: [],
                                                                boldBigBuckBunny: [],
                                                                boldBigBuckBunny_sbref: [],
                                                                boldPartlyCloudy: [],
                                                                boldPartlyCloudy_sbref: [],
                                                                boldSkewedGambling: [],
                                                                boldSkewedGambling_sbref: [],
                                                                fmap_magn: [],
                                                                fmap_phase: [],
                                                                t1w: [],
                                                                dwi: [],
                                                                dwi_sbref: []
                                                                }

    for s in seqinfo:
        """
        The namedtuple `s` contains the following fields:

        * total_files_till_now
        * example_dcm_file
        * series_id
        * dcm_dir_name
        * unspecified2
        * unspecified3
        * dim1
        * dim2
        * dim3
        * dim4
        * TR
        * TE
        * protocol_name
        * is_motion_corrected
        * is_derived
        * patient_id
        * study_description
        * referring_physician_name
        * series_description
        * image_type
        """
        if ("emotion" in s.protocol_name and s.dim4 == 3) :
            info[boldHariri_sbref].append(s.series_id)
        if ("emotion" in s.protocol_name and s.dim4 == 1257) :
            info[boldHariri].append(s.series_id)
        if ("film_1" in s.protocol_name and s.dim4 == 3) :
            info[boldBigBuckBunny_sbref].append(s.series_id)
        if ("film_1" in s.protocol_name and s.dim4 == 999) :
            info[boldBigBuckBunny].append(s.series_id)
        if ("film_2" in s.protocol_name and s.dim4 == 3) :
            info[boldPartlyCloudy_sbref].append(s.series_id)
        if ("film_2" in s.protocol_name and s.dim4 == 726) :
            info[boldPartlyCloudy].append(s.series_id)
        if ("gambling" in s.protocol_name and s.dim4 == 3) :
            info[boldSkewedGambling_sbref].append(s.series_id)
        if ("gambling" in s.protocol_name and s.dim4 > 1400 ) :
            info[boldSkewedGambling].append(s.series_id)
        if ("gre_field_mapping" in s.protocol_name and s.dim4 == 106):
            info[fmap_magn].append(s.series_id)
        if ("gre_field_mapping" in s.protocol_name and s.dim4 == 53):
            info[fmap_phase].append(s.series_id)
        if ("T1_SAG" in s.protocol_name) and ('NORM' in s.image_type) :
            info[t1w].append(s.series_id)
        if ("dmri" in s.protocol_name and "b2700" in s.series_description and "PA" in s.series_description and s.dim4 == 1):
            info[dwi_sbref].append({"item": s.series_id, "acq": "b2700", 'dir': "PA"})
        if ("dmri" in s.protocol_name and "b2700" in s.series_description and "PA" in s.series_description and s.dim4 == 151):
            info[dwi].append({"item": s.series_id, "acq": "b2700", 'dir': "PA"})
        if ("dmri" in s.protocol_name and "b2700" in s.series_description and "AP" in s.series_description and s.dim4 == 1):
            info[dwi_sbref].append({"item": s.series_id, "acq": "b2700", 'dir': "AP"})
        if ("dmri" in s.protocol_name and "b2700" in s.series_description and "AP" in s.series_description and s.dim4 == 151):
            info[dwi].append({"item": s.series_id, "acq": "b2700", 'dir': "AP"})
        if ("dmri" in s.protocol_name and "b0" in s.series_description and "AP" in s.series_description and s.dim4 == 1):
            info[dwi_sbref].append({"item": s.series_id, "acq": "b0", 'dir': "AP"})
        if ("dmri" in s.protocol_name and "b0" in s.series_description and "AP" in s.series_description and s.dim4 == 7):
            info[dwi].append({"item": s.series_id, "acq": "b0", 'dir': "AP"})
        if ("dmri" in s.protocol_name and "b0" in s.series_description and "PA" in s.series_description and s.dim4 == 1):
            info[dwi_sbref].append({"item": s.series_id, "acq": "b0", 'dir': "PA"})
        if ("dmri" in s.protocol_name and "b0" in s.series_description and "PA" in s.series_description and s.dim4 == 7):
            info[dwi].append({"item": s.series_id, "acq": "b0", 'dir': "PA"})
    return info
```

## step3_heudiconv.sh

``` {.bash filename="step3_heudiconv.sh" code-line-numbers="true" code-fold="true"}
#!/bin/bash

###############################
### Original script from Arnaud Fournel, PhD, NeuroPop team, CRNL, Lyon
### arnaud.fournel @ inserm.fr
### 
### Adapted for the CRNL study
### by Gaelle Leroux, PhD
### and Isabelle Faillenot, PhD
###
### Autumn 2020, Lyon
### gaelle.leroux @ cnrs.fr
###
### launched by: sbatch step3_heudiconv.sh ${subject} ${session_name} ${WD} ${my_study}
###
###############################
#
### The SBATCH directives (line 39 to be revised only):

### Your job name displayed by the queue
### use "squeue" command in a terminal to see it
#SBATCH --job-name=HeuDC_3

### Specify output and error files
### %A for job array's master job allocation number.
### or %a for job array ID (index) number
#SBATCH --output=out_step3_%A.log
#SBATCH --error=err_step3_%A.log

### Specify the number of tasks, CPU per task and buffer size to be used
### (up to 4 CPU/task to reach optimal power)
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=10G

### Send email for which step: NONE, BEGIN, END, FAIL, REQUEUE, ALL
#SBATCH --mail-type=ALL
#SBATCH --mail-user=${firstName}.${lastName}@univ-lyon1.fr

### The size of the participant table to be run
### SBATCH --array=1-1

### If you want to launch your script on a specific node of the cluser
### If yes, uncomment the appropirate line(s) 
###SBATCH --exclude=node9
###SBATCH --nodelist=node10

# End of the SBATCH directives
###############################
#
# Paths of the study
subject=$1
session_name=$2
WD=$3
my_study=$4
STUDY=${WD}/${my_study}
DATA_DIRECTORY="${STUDY}/data"
rm ${STUDY}/data/bids/${subject}/${session_name}/.heudiconv/*.edit.txt
#
HEUDICONV_SINGULARITY_IMG="/mnt/data/soft/Images/heudiconv_1.1.0.sif"
#
## Comment: Parse the participants.tsv file and extract one subject ID from the line corresponding to this SLURM task.
## Comment: 1 line below to uncomment if you want to launch the script for all the subjects listed in ${DATA_DIRECTORY}/participants.tsv
#subject=$( sed -n -E "$((${SLURM_ARRAY_TASK_ID} + 1))s/sub-(\S*)\>.*/\1/gp" ${DATA_DIRECTORY}/participants.tsv )
#
# To be printed in the out_*.log file :
echo "#########################################################################"
echo "User:" $USER
echo "#"
echo "SLURM_SUBMITING_DIRECTORY:" $SLURM_SUBMIT_DIR
echo "SLURM_JOB_NODELIST:" $SLURM_NODELIST
echo "SLURM_JOB_NAME:" $SLURM_JOB_NAME
echo "SLURM_JOB_ID:" $SLURM_JOBID
echo "SLURM_ARRAY_TASK_ID:" $SLURM_ARRAY_TASK_ID
echo "SLURM_NTASKS:" $SLURM_NTASKS
echo "#"
echo "Step 3: conversion of DICOM to NIFTI with BIDS standards using HeuDiConv"
echo "#"
echo "Subject processed:" ${subject}
echo "Session processed:" ${session_name}
echo "#"
echo "Job STARTED @ $(date)"heuristic.py
echo "#"
#
# STEP 3/3: conversion of DICOM to NIFTII using dcm2niix and to a BIDS standard organisation
# submission to slurm and running the HeuDiCon singularity image
#
# Check the "-d" path (line 87) pointing at the dicom files
#
# Compose the command line
cmd="srun singularity run \
    --cleanenv \
    -B ${DATA_DIRECTORY}:/base -B ${STUDY}:/study \
    ${HEUDICONV_SINGULARITY_IMG} \
    -d /base/dicom/{subject}/{session}/scans/*/resources/DICOM/files/*.??? \
    -s ${subject} \
    --ses ${session_name} \
    -f /study/code/heuristic.py \
    -c dcm2niix -b \
    -o /base/bids \
    --minmeta \
    --overwrite"
#
# Setup done, run the command
echo Running task ${SLURM_ARRAY_TASK_ID}
echo "Command line used (example of the last subject processed)"
echo $cmd
#
echo "#"
#
eval $cmd
exitcode=$?
#
echo "#"
echo "Job STOPPED @ $(date)"
echo "#"
echo "If STEP 3 successful: check the text files & folders (.heudiconv and sub-{$1}) in" 
echo ${DATA_DIRECTORY}"/bids"
echo "Edit the file" ${DATA_DIRECTORY}"/dataset_description.json"
echo "Create events.tsv file for each func/*.json file"
echo "#"
echo "Then, validate your nifti folder using online BIDS VALIDATOR: https://bids-standard.github.io/bids-validator/"
echo "############################################################################################"
echo "#"
# Output results to a table
echo "sub-${subject}    ${SLURM_ARRAY_TASK_ID}  $exitcode" >> ${SLURM_JOB_NAME}.step3.${SLURM_ARRAY_JOB_ID}.tsv
echo Finished tasks ${SLURM_ARRAY_TASK_ID} with exit code $exitcode
exit $exitcode
```
